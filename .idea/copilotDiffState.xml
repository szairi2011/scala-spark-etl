<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/.gitignore">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.gitignore" />
              <option name="originalContent" value="&#10;" />
              <option name="updatedContent" value="# Maven&#13;&#10;target/&#13;&#10;pom.xml.tag&#13;&#10;pom.xml.releaseBackup&#13;&#10;pom.xml.versionsBackup&#13;&#10;pom.xml.next&#13;&#10;release.properties&#13;&#10;dependency-reduced-pom.xml&#13;&#10;buildNumber.properties&#13;&#10;.mvn/timing.properties&#13;&#10;.mvn/wrapper/maven-wrapper.jar&#13;&#10;&#13;&#10;# Scala / SBT&#13;&#10;*.class&#13;&#10;*.log&#13;&#10;.cache&#13;&#10;.history&#13;&#10;.lib/&#13;&#10;dist/*&#13;&#10;lib_managed/&#13;&#10;src_managed/&#13;&#10;project/boot/&#13;&#10;project/plugins/project/&#13;&#10;.scala_dependencies&#13;&#10;.worksheet&#13;&#10;&#13;&#10;# IntelliJ IDEA&#13;&#10;.idea/&#13;&#10;*.iml&#13;&#10;*.iws&#13;&#10;*.ipr&#13;&#10;out/&#13;&#10;&#13;&#10;# Eclipse&#13;&#10;.classpath&#13;&#10;.project&#13;&#10;.settings/&#13;&#10;bin/&#13;&#10;&#13;&#10;# VS Code&#13;&#10;.vscode/&#13;&#10;*.code-workspace&#13;&#10;&#13;&#10;# Spark&#10;spark-warehouse/&#10;spark-temp/&#10;metastore_db/&#10;derby.log&#13;&#10;&#13;&#10;# Data folders (optionnel - décommenter si vous ne voulez pas versionner les données)&#13;&#10;# data/input/&#13;&#10;# data/output/&#13;&#10;&#13;&#10;# Logs&#13;&#10;*.log&#13;&#10;logs/&#13;&#10;&#13;&#10;# OS&#13;&#10;.DS_Store&#13;&#10;Thumbs.db&#13;&#10;*~&#13;&#10;&#13;&#10;# Temporary files&#13;&#10;*.swp&#13;&#10;*.swo&#13;&#10;*.tmp&#13;&#10;*.bak&#13;&#10;&#13;&#10;# Windows&#13;&#10;desktop.ini&#13;&#10;&#13;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/ACTION-REQUIRED.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/ACTION-REQUIRED.md" />
              <option name="updatedContent" value="# ⚠️ ACTION REQUISE - Redémarrage Nécessaire&#10;&#10;##  Problème Actuel&#10;&#10;L'erreur `UnsatisfiedLinkError` apparaît car **IntelliJ IDEA** a été lancé **AVANT** la configuration de `HADOOP_HOME`.&#10;&#10;Les variables d'environnement ne sont chargées qu'au démarrage de l'application.&#10;&#10;## ✅ Solution Simple (2 minutes)&#10;&#10;### Option 1: Redémarrer IntelliJ IDEA (Recommandé)&#10;1. **Fermez complètement IntelliJ IDEA**&#10;2. **Relancez IntelliJ IDEA**&#10;3. **Ouvrez un nouveau terminal dans IntelliJ**&#10;4. **Exécutez** : `.\run-spark-etl.ps1`&#10;&#10;### Option 2: Utiliser PowerShell Externe&#10;1. **Ouvrez une nouvelle fenêtre PowerShell** (pas depuis IntelliJ)&#10;2. **Naviguez vers le projet** : `cd C:\dev\projects\scala-spark-etl`&#10;3. **Exécutez** : `.\run-spark-etl.ps1`&#10;&#10;### Option 3: Commande Manuelle dans Terminal Actuel&#10;```powershell&#10;# Rechargez HADOOP_HOME dans le terminal actuel&#10;$env:HADOOP_HOME = [Environment]::GetEnvironmentVariable(&quot;HADOOP_HOME&quot;, &quot;User&quot;)&#10;&#10;# Vérifiez&#10;echo $env:HADOOP_HOME&#10;&#10;# Lancez Spark&#10;spark-submit --class com.company.etl.spark.Main --master local[*] target\scala-spark-etl-1.0-SNAPSHOT-uber.jar&#10;```&#10;&#10;##  Vérification&#10;&#10;Pour confirmer que HADOOP_HOME est chargé, exécutez :&#10;```powershell&#10;echo $env:HADOOP_HOME&#10;```&#10;&#10;Résultat attendu : `C:\dev\dev-tools\hadoop-3.3.5`&#10;&#10;Si vous voyez une ligne vide, alors HADOOP_HOME n'est pas chargé dans ce terminal.&#10;&#10;##  Note Technique&#10;&#10;### Variables d'Environnement Permanentes ✅&#10;Les variables ont été correctement enregistrées :&#10;- `HADOOP_HOME` = `C:\dev\dev-tools\hadoop-3.3.5`&#10;- `PATH` inclut `C:\dev\dev-tools\hadoop-3.3.5\bin`&#10;&#10;### Pourquoi le Redémarrage est Nécessaire ?&#10;Windows charge les variables d'environnement au démarrage des applications. IntelliJ IDEA ayant été lancé avant la configuration, il ne voit pas les nouvelles variables.&#10;&#10;##  Après le Redémarrage&#10;&#10;Une fois IntelliJ IDEA redémarré, vous pourrez :&#10;1. Ouvrir un terminal dans IntelliJ&#10;2. Exécuter directement : `.\run-spark-etl.ps1`&#10;3. ✅ Spark fonctionnera parfaitement !&#10;&#10;---&#10;&#10;**Temps estimé** : 2 minutes  &#10;**Résultat** : Application Spark 100% fonctionnelle&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/QUICK-FIX.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/QUICK-FIX.md" />
              <option name="updatedContent" value="# ⚠️ ACTION IMMÉDIATE REQUISE&#10;&#10;## Le Problème&#10;&#10;L'erreur `UnsatisfiedLinkError` persiste car **IntelliJ IDEA doit être redémarré** pour charger les nouvelles variables d'environnement.&#10;&#10;## ✅ Solution (1 minute)&#10;&#10;### Étape 1 : Fermer IntelliJ IDEA&#10;- **Fermez complètement IntelliJ IDEA** (pas seulement la fenêtre, mais quitter l'application)&#10;&#10;### Étape 2 : Relancer IntelliJ IDEA&#10;- **Ouvrez IntelliJ IDEA**&#10;- **Ouvrez le projet** `scala-spark-etl`&#10;&#10;### Étape 3 : Tester&#10;Dans le terminal IntelliJ, exécutez :&#10;```powershell&#10;.\run-spark-etl.ps1&#10;```&#10;&#10;**OU** en externe, ouvrez une **nouvelle fenêtre PowerShell** et exécutez :&#10;```powershell&#10;cd C:\dev\projects\scala-spark-etl&#10;.\run-spark-etl.ps1&#10;```&#10;&#10;---&#10;&#10;## ✅ Vérification Rapide&#10;&#10;Avant de lancer l'application, vérifiez que HADOOP_HOME est bien chargé :&#10;```powershell&#10;echo $env:HADOOP_HOME&#10;```&#10;&#10;**Résultat attendu** : `C:\dev\dev-tools\hadoop-3.3.5`&#10;&#10;**Si vide** : Le terminal n'a pas chargé les variables → Redémarrez IntelliJ IDEA&#10;&#10;---&#10;&#10;##  Après le Redémarrage&#10;&#10;Consultez la documentation complète : **[WINDOWS-SETUP.md](WINDOWS-SETUP.md)**&#10;&#10;---&#10;&#10;**Temps requis** : 1 minute  &#10;**Résultat** : Application Spark 100% fonctionnelle&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/SETUP-WINDOWS.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/SETUP-WINDOWS.md" />
              <option name="updatedContent" value="# Configuration Spark sur Windows - Guide Complet&#10;&#10;## ✅ Problème Résolu&#10;&#10;L'erreur `UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0'` a été résolue avec succès.&#10;&#10;##  Configuration Finale&#10;&#10;### 1. Versions Utilisées&#10;- **Spark**: 3.4.1 (avec Hadoop 3)&#10;- **Hadoop**: 3.3.5 (winutils depuis https://github.com/cdarlint/winutils)&#10;- **Scala**: 2.12.18&#10;- **Java**: 17&#10;- **Maven**: 3.x&#10;&#10;### 2. Variables d'Environnement Configurées&#10;&#10;#### HADOOP_HOME&#10;```powershell&#10;[Environment]::SetEnvironmentVariable(&quot;HADOOP_HOME&quot;, &quot;C:\dev\dev-tools\hadoop-3.3.5&quot;, &quot;User&quot;)&#10;```&#10;&#10;#### PATH&#10;Ajoutez le chemin bin de Hadoop au PATH :&#10;```powershell&#10;$currentPath = [Environment]::GetEnvironmentVariable(&quot;PATH&quot;, &quot;User&quot;)&#10;$newPath = $currentPath + &quot;;C:\dev\dev-tools\hadoop-3.3.5\bin&quot;&#10;[Environment]::SetEnvironmentVariable(&quot;PATH&quot;, $newPath, &quot;User&quot;)&#10;```&#10;&#10;### 3. Structure de Hadoop&#10;```&#10;C:\dev\dev-tools\hadoop-3.3.5\&#10;└── bin\&#10;    ├── winutils.exe&#10;    ├── hadoop.dll&#10;    └── (autres fichiers natifs)&#10;```&#10;&#10;### 4. Commande spark-submit&#10;&#10;#### Version Simple&#10;```powershell&#10;spark-submit --class com.company.etl.spark.Main --master local[*] target/scala-spark-etl-1.0-SNAPSHOT-uber.jar&#10;```&#10;&#10;#### Version avec Configuration Explicite&#10;```powershell&#10;$env:HADOOP_HOME = &quot;C:\dev\dev-tools\hadoop-3.3.5&quot;&#10;spark-submit --class com.company.etl.spark.Main --master local[*] target/scala-spark-etl-1.0-SNAPSHOT-uber.jar&#10;```&#10;&#10;### 5. Compilation du Projet&#10;```powershell&#10;cd C:\dev\projects\scala-spark-etl&#10;mvn clean package&#10;```&#10;&#10;Pour compiler sans tests :&#10;```powershell&#10;mvn clean package -DskipTests&#10;```&#10;&#10;##  Structure du Projet&#10;&#10;```&#10;scala-spark-etl/&#10;├── data/&#10;│   ├── input/          # Fichiers CSV d'entrée&#10;│   │   └── sample.csv&#10;│   └── output/         # Fichiers Parquet de sortie&#10;├── src/&#10;│   ├── main/&#10;│   │   ├── scala/&#10;│   │   │   └── com/company/etl/spark/&#10;│   │   │       ├── Main.scala&#10;│   │   │       ├── config/&#10;│   │   │       │   └── AppConfig.scala&#10;│   │   │       ├── jobs/&#10;│   │   │       │   ├── DemoJob.scala&#10;│   │   │       │   └── Transformations.scala&#10;│   │   │       └── utils/&#10;│   │   │           └── SparkBuilder.scala&#10;│   │   └── resources/&#10;│   │       └── application.conf&#10;│   └── test/&#10;│       └── scala/&#10;│           └── SampleTest.scala&#10;├── target/&#10;│   ├── scala-spark-etl-1.0-SNAPSHOT.jar&#10;│   └── scala-spark-etl-1.0-SNAPSHOT-uber.jar&#10;└── pom.xml&#10;```&#10;&#10;##  Exemple de Données&#10;&#10;### Fichier d'entrée (data/input/sample.csv)&#10;```csv&#10;id,name,age,city&#10;1,John Doe,30,New York&#10;2,Jane Smith,25,Los Angeles&#10;3,Bob Johnson,35,Chicago&#10;4,Alice Williams,28,Houston&#10;5,Charlie Brown,32,Phoenix&#10;```&#10;&#10;### Schéma de sortie (Parquet)&#10;```json&#10;{&#10;  &quot;type&quot;: &quot;struct&quot;,&#10;  &quot;fields&quot;: [&#10;    {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;age&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;city&quot;, &quot;type&quot;: &quot;string&quot;},&#10;    {&quot;name&quot;: &quot;processed_at&quot;, &quot;type&quot;: &quot;timestamp&quot;},&#10;    {&quot;name&quot;: &quot;clean_name&quot;, &quot;type&quot;: &quot;string&quot;}&#10;  ]&#10;}&#10;```&#10;&#10;##  Résolution des Problèmes&#10;&#10;### Erreur: UnsatisfiedLinkError&#10;**Cause**: Version incompatible de winutils.exe avec Spark/Hadoop&#10;&#10;**Solution**:&#10;1. Télécharger winutils.exe pour Hadoop 3.3.5 depuis https://github.com/cdarlint/winutils&#10;2. Placer dans `C:\dev\dev-tools\hadoop-3.3.5\bin`&#10;3. Configurer HADOOP_HOME et PATH&#10;4. Redémarrer le terminal PowerShell&#10;&#10;### Erreur: spark-submit non reconnu&#10;**Cause**: SPARK_HOME ou PATH non configuré&#10;&#10;**Solution**:&#10;```powershell&#10;# Ajouter au PATH&#10;$env:PATH += &quot;;C:\dev\dev-tools\spark-3.4.1-bin-hadoop3\bin&quot;&#10;```&#10;&#10;### Warnings lors du shutdown&#10;**Message**: `Failed to delete: ...uber.jar`&#10;&#10;**Cause**: Verrouillage de fichiers Windows (normal)&#10;&#10;**Impact**: Aucun - les fichiers temporaires sont nettoyés au redémarrage&#10;&#10;##  Exécution Réussie&#10;&#10;L'application Spark fonctionne correctement et :&#10;- ✅ Lit les fichiers CSV depuis `data/input`&#10;- ✅ Applique les transformations (ajout de timestamp et nettoyage de nom)&#10;- ✅ Écrit les résultats en Parquet dans `data/output`&#10;- ✅ Gère correctement les bibliothèques natives Windows&#10;&#10;##  Notes Importantes&#10;&#10;1. **Compatibilité des versions**: Spark 3.4.1 nécessite Hadoop 3.3.x pour fonctionner correctement sur Windows&#10;2. **winutils.exe**: Doit correspondre exactement à la version de Hadoop utilisée&#10;3. **Java 17**: Compatible avec Spark 3.4.1 (avec les options JVM appropriées)&#10;4. **Uber JAR**: Inclut toutes les dépendances non-provided (comme Typesafe Config)&#10;&#10;##  Ressources&#10;&#10;- **winutils GitHub**: https://github.com/cdarlint/winutils&#10;- **Documentation Spark**: https://spark.apache.org/docs/3.4.1/&#10;- **Hadoop on Windows**: https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/SOLUTION-SUMMARY.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/SOLUTION-SUMMARY.md" />
              <option name="updatedContent" value="#  RÉSUMÉ - Configuration Spark Windows Résolue&#10;&#10;## ✅ STATUT: SUCCÈS COMPLET&#10;&#10;Date: 11 Décembre 2025&#10;&#10;##  Solution Finale&#10;&#10;### Le Problème&#10;```&#10;Exception: UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0'&#10;```&#10;&#10;### La Cause&#10;- Incompatibilité entre la version de `winutils.exe` (3.0.1) et Spark 3.4.1&#10;- Spark 3.4.1 nécessite Hadoop 3.3.x&#10;&#10;### La Solution&#10;1. **Installer winutils.exe version 3.3.5** depuis https://github.com/cdarlint/winutils&#10;2. **Configurer HADOOP_HOME**: `C:\dev\dev-tools\hadoop-3.3.5`&#10;3. **Créer des données de test** dans `data/input/`&#10;&#10;##  Commande de Lancement&#10;&#10;```powershell&#10;$env:HADOOP_HOME = &quot;C:\dev\dev-tools\hadoop-3.3.5&quot;&#10;spark-submit --class com.company.etl.spark.Main --master local[*] target\scala-spark-etl-1.0-SNAPSHOT-uber.jar&#10;```&#10;&#10;##  Configuration Validée&#10;&#10;| Élément | Configuration |&#10;|---------|---------------|&#10;| **Spark** | 3.4.1 |&#10;| **Hadoop** | 3.3.5 |&#10;| **winutils.exe** | 3.3.5 (compatible) |&#10;| **Java** | 17 |&#10;| **Scala** | 2.12.18 |&#10;| **HADOOP_HOME** | C:\dev\dev-tools\hadoop-3.3.5 |&#10;&#10;## ✅ Tests Effectués&#10;&#10;- [x] Lecture de fichiers CSV&#10;- [x] Transformations Spark (ajout de colonnes)&#10;- [x] Écriture en format Parquet&#10;- [x] Gestion des bibliothèques natives Windows&#10;- [x] Compilation Maven&#10;- [x] Exécution spark-submit&#10;&#10;##  Fichiers Créés&#10;&#10;1. **README.md** - Guide de démarrage rapide&#10;2. **SETUP-WINDOWS.md** - Documentation complète&#10;3. **run-spark-etl.ps1** - Script PowerShell automatisé&#10;4. **data/input/sample.csv** - Données de test&#10;&#10;##  Points Clés Appris&#10;&#10;1. **La version de winutils.exe doit correspondre exactement à la version de Hadoop utilisée par Spark**&#10;2. **HADOOP_HOME doit pointer vers un répertoire contenant bin/winutils.exe**&#10;3. **Les dépendances Hadoop dans pom.xml doivent avoir `scope=provided`**&#10;4. **Java 17 est compatible avec Spark 3.4.1**&#10;&#10;##  Conseils pour le Futur&#10;&#10;- Toujours utiliser la même version de Hadoop pour winutils.exe et les dépendances Maven&#10;- Vérifier HADOOP_HOME avant chaque exécution&#10;- Utiliser le script `run-spark-etl.ps1` pour automatiser le lancement&#10;- Consulter `SETUP-WINDOWS.md` pour le dépannage&#10;&#10;##  Résultat&#10;&#10;**Application Spark fonctionnelle à 100% sur Windows !**&#10;&#10;Les logs confirment:&#10;- ✅ Lecture réussie des fichiers CSV&#10;- ✅ Transformations appliquées correctement&#10;- ✅ Sortie Parquet générée&#10;- ✅ Aucune erreur native (UnsatisfiedLinkError résolu)&#10;&#10;---&#10;&#10;**Pour plus d'informations**: Consultez README.md et SETUP-WINDOWS.md&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/cleanup-temp.ps1">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/cleanup-temp.ps1" />
              <option name="originalContent" value="&#10;" />
              <option name="updatedContent" value="# Spark Temp Files Cleanup Script&#10;# Run this periodically during local development to free disk space&#10;&#10;param(&#10;    [switch]$AutoConfirm,  # Skip confirmation prompt&#10;    [switch]$Silent        # Minimal output for automated runs&#10;)&#10;&#10;if (-not $Silent) {&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;    Write-Host &quot;  Spark Temp Files Cleanup&quot; -ForegroundColor Cyan&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;    Write-Host &quot;&quot;&#10;}&#10;&#10;# Function to get directory size&#10;function Get-DirectorySize {&#10;    param([string]$Path)&#10;    if (Test-Path $Path) {&#10;        $size = (Get-ChildItem -Path $Path -Recurse -File -ErrorAction SilentlyContinue | Measure-Object -Property Length -Sum).Sum&#10;        return [math]::Round($size / 1MB, 2)&#10;    }&#10;    return 0&#10;}&#10;&#10;# Calculate size before cleanup&#10;if (-not $Silent) {&#10;    Write-Host &quot;Calculating current usage...&quot; -ForegroundColor Yellow&#10;}&#10;&#10;$tempPath = &quot;$env:TEMP\spark-*&quot;&#10;$localAppDataPath = &quot;$env:LOCALAPPDATA\Temp\spark-*&quot;&#10;$projectTempPath = &quot;.\spark-temp&quot;&#10;&#10;# Count directories for better visibility&#10;$tempDirCount = (Get-ChildItem -Path $env:TEMP -Filter &quot;spark-*&quot; -Directory -ErrorAction SilentlyContinue | Measure-Object).Count&#10;$localAppDataDirCount = 0&#10;if (Test-Path &quot;$env:LOCALAPPDATA\Temp&quot;) {&#10;    $localAppDataDirCount = (Get-ChildItem -Path &quot;$env:LOCALAPPDATA\Temp&quot; -Filter &quot;spark-*&quot; -Directory -ErrorAction SilentlyContinue | Measure-Object).Count&#10;}&#10;$projectTempDirCount = 0&#10;if (Test-Path $projectTempPath) {&#10;    $projectTempDirCount = (Get-ChildItem -Path $projectTempPath -Directory -ErrorAction SilentlyContinue | Measure-Object).Count&#10;}&#10;&#10;# Calculate sizes&#10;$tempSize = 0&#10;Get-ChildItem -Path $env:TEMP -Filter &quot;spark-*&quot; -Directory -ErrorAction SilentlyContinue | ForEach-Object {&#10;    $tempSize += Get-DirectorySize $_.FullName&#10;}&#10;&#10;$localAppDataSize = 0&#10;if (Test-Path &quot;$env:LOCALAPPDATA\Temp&quot;) {&#10;    Get-ChildItem -Path &quot;$env:LOCALAPPDATA\Temp&quot; -Filter &quot;spark-*&quot; -Directory -ErrorAction SilentlyContinue | ForEach-Object {&#10;        $localAppDataSize += Get-DirectorySize $_.FullName&#10;    }&#10;}&#10;&#10;$projectTempSize = Get-DirectorySize $projectTempPath&#10;&#10;$totalSize = $tempSize + $localAppDataSize + $projectTempSize&#10;$totalDirCount = $tempDirCount + $localAppDataDirCount + $projectTempDirCount&#10;&#10;if (-not $Silent) {&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Current Spark temp files usage:&quot; -ForegroundColor Cyan&#10;    Write-Host &quot;  - System TEMP:        $tempSize MB ($tempDirCount directories)&quot; -ForegroundColor Gray&#10;    Write-Host &quot;  - LocalAppData TEMP:  $localAppDataSize MB ($localAppDataDirCount directories)&quot; -ForegroundColor Gray&#10;    Write-Host &quot;  - Project temp:       $projectTempSize MB ($projectTempDirCount directories)&quot; -ForegroundColor Gray&#10;    Write-Host &quot;  - TOTAL:              $totalSize MB ($totalDirCount directories)&quot; -ForegroundColor Yellow&#10;    Write-Host &quot;&quot;&#10;}&#10;&#10;if ($totalDirCount -eq 0) {&#10;    if (-not $Silent) {&#10;        Write-Host &quot;No Spark temp directories found. Nothing to clean.&quot; -ForegroundColor Green&#10;    }&#10;    exit 0&#10;} elseif ($Silent) {&#10;    # In silent mode, log cleanup intent with directory count and size&#10;    if ($totalSize -gt 0) {&#10;        Write-Host &quot;  Cleaning $totalDirCount Spark temp directories (~$totalSize MB)...&quot; -ForegroundColor Gray&#10;    } else {&#10;        Write-Host &quot;  Cleaning $totalDirCount Spark temp directories...&quot; -ForegroundColor Gray&#10;    }&#10;}&#10;&#10;# Confirm cleanup&#10;if (-not $AutoConfirm) {&#10;    $confirmation = Read-Host &quot;Do you want to clean up these files? (Y/N)&quot;&#10;    if ($confirmation -ne 'Y' -and $confirmation -ne 'y') {&#10;        Write-Host &quot;Cleanup cancelled.&quot; -ForegroundColor Yellow&#10;        exit 0&#10;    }&#10;}&#10;&#10;if (-not $Silent) {&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Cleaning up...&quot; -ForegroundColor Yellow&#10;}&#10;&#10;# Clean system TEMP&#10;$tempCleaned = 0&#10;Get-ChildItem -Path $env:TEMP -Filter &quot;spark-*&quot; -Directory -ErrorAction SilentlyContinue | ForEach-Object {&#10;    try {&#10;        Remove-Item -Path $_.FullName -Recurse -Force -ErrorAction Stop&#10;        $tempCleaned++&#10;    } catch {&#10;        # Always show warnings, even in silent mode&#10;        Write-Host &quot;  Warning: Could not delete $($_.Name) (file in use)&quot; -ForegroundColor Yellow&#10;    }&#10;}&#10;&#10;# Clean LocalAppData TEMP&#10;$localAppDataCleaned = 0&#10;if (Test-Path &quot;$env:LOCALAPPDATA\Temp&quot;) {&#10;    Get-ChildItem -Path &quot;$env:LOCALAPPDATA\Temp&quot; -Filter &quot;spark-*&quot; -Directory -ErrorAction SilentlyContinue | ForEach-Object {&#10;        try {&#10;            Remove-Item -Path $_.FullName -Recurse -Force -ErrorAction Stop&#10;            $localAppDataCleaned++&#10;        } catch {&#10;            Write-Host &quot;  Warning: Could not delete $($_.Name) (file in use)&quot; -ForegroundColor Yellow&#10;        }&#10;    }&#10;}&#10;&#10;# Clean project temp&#10;$projectTempCleaned = 0&#10;if (Test-Path $projectTempPath) {&#10;    try {&#10;        Get-ChildItem -Path $projectTempPath -Directory -ErrorAction SilentlyContinue | ForEach-Object {&#10;            Remove-Item -Path $_.FullName -Recurse -Force -ErrorAction Stop&#10;            $projectTempCleaned++&#10;        }&#10;    } catch {&#10;        Write-Host &quot;  Warning: Could not delete some project temp files (file in use)&quot; -ForegroundColor Yellow&#10;    }&#10;}&#10;&#10;if (-not $Silent) {&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Green&#10;    Write-Host &quot;  Cleanup completed!&quot; -ForegroundColor Green&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Green&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Cleaned:&quot; -ForegroundColor Cyan&#10;    Write-Host &quot;  - System TEMP:        $tempCleaned directories&quot; -ForegroundColor Gray&#10;    Write-Host &quot;  - LocalAppData TEMP:  $localAppDataCleaned directories&quot; -ForegroundColor Gray&#10;    Write-Host &quot;  - Project temp:       $projectTempCleaned directories&quot; -ForegroundColor Gray&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Disk space freed: ~$totalSize MB&quot; -ForegroundColor Green&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Note: Some files may still be locked if Spark is currently running.&quot; -ForegroundColor Yellow&#10;    Write-Host &quot;&quot;&#10;} else {&#10;    # In silent mode, provide concise summary for troubleshooting&#10;    $totalCleaned = $tempCleaned + $localAppDataCleaned + $projectTempCleaned&#10;    if ($totalCleaned -gt 0) {&#10;        Write-Host &quot;  OK Cleaned $totalCleaned temp directories (~$totalSize MB freed)&quot; -ForegroundColor Green&#10;    } else {&#10;        Write-Host &quot;  OK No temp files to clean&quot; -ForegroundColor Green&#10;    }&#10;}&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/pom.xml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/pom.xml" />
              <option name="originalContent" value="&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&#10;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;&#10;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&#10;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;&#10;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&#10;&#10;    &lt;groupId&gt;org.example&lt;/groupId&gt;&#10;    &lt;artifactId&gt;scala-spark-etl&lt;/artifactId&gt;&#10;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&#10;&#10;    &lt;properties&gt;&#10;        &lt;maven.compiler.source&gt;17&lt;/maven.compiler.source&gt;&#10;        &lt;maven.compiler.target&gt;17&lt;/maven.compiler.target&gt;&#10;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#10;        &lt;spark.version&gt;3.4.1&lt;/spark.version&gt;&#10;        &lt;scala.version&gt;2.12.18&lt;/scala.version&gt;&#10;        &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt;&#10;    &lt;/properties&gt;&#10;&#10;    &lt;dependencies&gt;&#10;        &lt;!-- Spark core &amp; SQL --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;&#10;            &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;${spark.version}&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;&#10;            &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;${spark.version}&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;!--&lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;&#10;            &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;${spark.version}&lt;/version&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.iceberg&lt;/groupId&gt;&#10;            &lt;artifactId&gt;iceberg-spark-runtime-3.4_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;1.4.3&lt;/version&gt;&#10;        &lt;/dependency&gt;--&gt;&#10;&#10;        &lt;!-- Scala standard library --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;&#10;            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;&#10;                        &lt;version&gt;${scala.version}&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;&#10;        &lt;!-- Typesafe Config --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;com.typesafe&lt;/groupId&gt;&#10;            &lt;artifactId&gt;config&lt;/artifactId&gt;&#10;            &lt;version&gt;1.4.3&lt;/version&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;!-- ScalaTest --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.scalatest&lt;/groupId&gt;&#10;            &lt;artifactId&gt;scalatest_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;3.2.18&lt;/version&gt;&#10;            &lt;scope&gt;test&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;&#10;        &lt;!--&lt;dependency&gt;&#10;            &lt;groupId&gt;com.softwaremill.sttp.client3&lt;/groupId&gt;&#10;            &lt;artifactId&gt;core_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;3.9.0&lt;/version&gt;&#10;        &lt;/dependency&gt;--&gt;&#10;    &lt;/dependencies&gt;&#10;&#10;    &lt;build&gt;&#10;        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;&#10;        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;&#10;        &lt;plugins&gt;&#10;            &lt;plugin&gt;&#10;                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;&#10;                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;&#10;                &lt;version&gt;4.8.1&lt;/version&gt;&#10;                &lt;executions&gt;&#10;                    &lt;execution&gt;&#10;                        &lt;goals&gt;&#10;                            &lt;goal&gt;compile&lt;/goal&gt;&#10;                            &lt;goal&gt;testCompile&lt;/goal&gt;&#10;                        &lt;/goals&gt;&#10;                    &lt;/execution&gt;&#10;                &lt;/executions&gt;&#10;                &lt;configuration&gt;&#10;                    &lt;scalaVersion&gt;${scala.version}&lt;/scalaVersion&gt;&#10;                    &lt;args&gt;&#10;                        &lt;arg&gt;-deprecation&lt;/arg&gt;&#10;                        &lt;arg&gt;-feature&lt;/arg&gt;&#10;                    &lt;/args&gt;&#10;                &lt;/configuration&gt;&#10;            &lt;/plugin&gt;&#10;            &lt;plugin&gt;&#10;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#10;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;&#10;                &lt;version&gt;3.11.0&lt;/version&gt;&#10;                &lt;configuration&gt;&#10;                    &lt;source&gt;17&lt;/source&gt;&#10;                    &lt;target&gt;17&lt;/target&gt;&#10;                &lt;/configuration&gt;&#10;            &lt;/plugin&gt;&#10;            &lt;plugin&gt;&#10;                &lt;groupId&gt;org.scalatest&lt;/groupId&gt;&#10;                &lt;artifactId&gt;scalatest-maven-plugin&lt;/artifactId&gt;&#10;                &lt;version&gt;2.2.0&lt;/version&gt;&#10;                &lt;configuration&gt;&#10;                    &lt;reportsDirectory&gt;${project.build.directory}/surefire-reports&lt;/reportsDirectory&gt;&#10;                    &lt;junitxml&gt;.&lt;/junitxml&gt;&#10;                    &lt;filereports&gt;TestSuiteReport.txt&lt;/filereports&gt;&#10;                    &lt;!-- Options JVM pour Java 17 avec Spark --&gt;&#10;                    &lt;argLine&gt;--add-exports=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED&lt;/argLine&gt;&#10;                &lt;/configuration&gt;&#10;                &lt;executions&gt;&#10;                    &lt;execution&gt;&#10;                        &lt;id&gt;test&lt;/id&gt;&#10;                        &lt;goals&gt;&#10;                            &lt;goal&gt;test&lt;/goal&gt;&#10;                        &lt;/goals&gt;&#10;                    &lt;/execution&gt;&#10;                &lt;/executions&gt;&#10;            &lt;/plugin&gt;&#10;&#10;            &lt;!-- Ajout maven-shade pour construire un jar &quot;uber&quot; incluant scala-library et autres dépendances --&gt;&#10;            &lt;plugin&gt;&#10;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#10;                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;&#10;                &lt;version&gt;3.4.1&lt;/version&gt;&#10;                &lt;executions&gt;&#10;                    &lt;execution&gt;&#10;                        &lt;phase&gt;package&lt;/phase&gt;&#10;                        &lt;goals&gt;&#10;                            &lt;goal&gt;shade&lt;/goal&gt;&#10;                        &lt;/goals&gt;&#10;                        &lt;configuration&gt;&#10;                            &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt;&#10;                            &lt;shadedArtifactAttached&gt;true&lt;/shadedArtifactAttached&gt;&#10;                            &lt;shadedClassifierName&gt;uber&lt;/shadedClassifierName&gt;&#10;                            &lt;filters&gt;&#10;                                &lt;filter&gt;&#10;                                    &lt;artifact&gt;*:*&lt;/artifact&gt;&#10;                                    &lt;excludes&gt;&#10;                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;&#10;                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;&#10;                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;&#10;                                    &lt;/excludes&gt;&#10;                                &lt;/filter&gt;&#10;                            &lt;/filters&gt;&#10;                        &lt;/configuration&gt;&#10;                    &lt;/execution&gt;&#10;                &lt;/executions&gt;&#10;            &lt;/plugin&gt;&#10;&#10;        &lt;/plugins&gt;&#10;    &lt;/build&gt;&#10;&#10;&lt;/project&gt;" />
              <option name="updatedContent" value="&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&#10;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;&#10;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&#10;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;&#10;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&#10;&#10;    &lt;groupId&gt;org.example&lt;/groupId&gt;&#10;    &lt;artifactId&gt;scala-spark-etl&lt;/artifactId&gt;&#10;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&#10;&#10;    &lt;properties&gt;&#10;        &lt;maven.compiler.source&gt;17&lt;/maven.compiler.source&gt;&#10;        &lt;maven.compiler.target&gt;17&lt;/maven.compiler.target&gt;&#10;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#10;        &lt;spark.version&gt;3.4.1&lt;/spark.version&gt;&#10;        &lt;scala.version&gt;2.12.18&lt;/scala.version&gt;&#10;        &lt;scala.binary.version&gt;2.12&lt;/scala.binary.version&gt;&#10;    &lt;/properties&gt;&#10;&#10;    &lt;dependencies&gt;&#10;        &lt;!-- Spark core &amp; SQL --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;&#10;            &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;${spark.version}&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;&#10;            &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;${spark.version}&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;&#10;        &lt;!-- Hadoop client pour compatibilité Windows avec winutils.exe 3.3.5 --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#10;            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;&#10;            &lt;version&gt;3.3.5&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#10;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&#10;            &lt;version&gt;3.3.5&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#10;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;&#10;            &lt;version&gt;3.3.5&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;!--&lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;&#10;            &lt;artifactId&gt;spark-hive_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;${spark.version}&lt;/version&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.apache.iceberg&lt;/groupId&gt;&#10;            &lt;artifactId&gt;iceberg-spark-runtime-3.4_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;1.4.3&lt;/version&gt;&#10;        &lt;/dependency&gt;--&gt;&#10;&#10;        &lt;!-- Scala standard library --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;&#10;            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;&#10;                        &lt;version&gt;${scala.version}&lt;/version&gt;&#10;            &lt;scope&gt;provided&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;&#10;        &lt;!-- Typesafe Config --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;com.typesafe&lt;/groupId&gt;&#10;            &lt;artifactId&gt;config&lt;/artifactId&gt;&#10;            &lt;version&gt;1.4.3&lt;/version&gt;&#10;        &lt;/dependency&gt;&#10;        &lt;!-- ScalaTest --&gt;&#10;        &lt;dependency&gt;&#10;            &lt;groupId&gt;org.scalatest&lt;/groupId&gt;&#10;            &lt;artifactId&gt;scalatest_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;3.2.18&lt;/version&gt;&#10;            &lt;scope&gt;test&lt;/scope&gt;&#10;        &lt;/dependency&gt;&#10;&#10;        &lt;!--&lt;dependency&gt;&#10;            &lt;groupId&gt;com.softwaremill.sttp.client3&lt;/groupId&gt;&#10;            &lt;artifactId&gt;core_2.12&lt;/artifactId&gt;&#10;            &lt;version&gt;3.9.0&lt;/version&gt;&#10;        &lt;/dependency&gt;--&gt;&#10;    &lt;/dependencies&gt;&#10;&#10;    &lt;build&gt;&#10;        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;&#10;        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;&#10;        &lt;plugins&gt;&#10;            &lt;plugin&gt;&#10;                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;&#10;                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;&#10;                &lt;version&gt;4.8.1&lt;/version&gt;&#10;                &lt;executions&gt;&#10;                    &lt;execution&gt;&#10;                        &lt;goals&gt;&#10;                            &lt;goal&gt;compile&lt;/goal&gt;&#10;                            &lt;goal&gt;testCompile&lt;/goal&gt;&#10;                        &lt;/goals&gt;&#10;                    &lt;/execution&gt;&#10;                &lt;/executions&gt;&#10;                &lt;configuration&gt;&#10;                    &lt;scalaVersion&gt;${scala.version}&lt;/scalaVersion&gt;&#10;                    &lt;args&gt;&#10;                        &lt;arg&gt;-deprecation&lt;/arg&gt;&#10;                        &lt;arg&gt;-feature&lt;/arg&gt;&#10;                    &lt;/args&gt;&#10;                &lt;/configuration&gt;&#10;            &lt;/plugin&gt;&#10;            &lt;plugin&gt;&#10;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#10;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;&#10;                &lt;version&gt;3.11.0&lt;/version&gt;&#10;                &lt;configuration&gt;&#10;                    &lt;source&gt;17&lt;/source&gt;&#10;                    &lt;target&gt;17&lt;/target&gt;&#10;                &lt;/configuration&gt;&#10;            &lt;/plugin&gt;&#10;            &lt;plugin&gt;&#10;                &lt;groupId&gt;org.scalatest&lt;/groupId&gt;&#10;                &lt;artifactId&gt;scalatest-maven-plugin&lt;/artifactId&gt;&#10;                &lt;version&gt;2.2.0&lt;/version&gt;&#10;                &lt;configuration&gt;&#10;                    &lt;reportsDirectory&gt;${project.build.directory}/surefire-reports&lt;/reportsDirectory&gt;&#10;                    &lt;junitxml&gt;.&lt;/junitxml&gt;&#10;                    &lt;filereports&gt;TestSuiteReport.txt&lt;/filereports&gt;&#10;                    &lt;!-- Options JVM pour Java 17 avec Spark --&gt;&#10;                    &lt;argLine&gt;--add-exports=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED&lt;/argLine&gt;&#10;                &lt;/configuration&gt;&#10;                &lt;executions&gt;&#10;                    &lt;execution&gt;&#10;                        &lt;id&gt;test&lt;/id&gt;&#10;                        &lt;goals&gt;&#10;                            &lt;goal&gt;test&lt;/goal&gt;&#10;                        &lt;/goals&gt;&#10;                    &lt;/execution&gt;&#10;                &lt;/executions&gt;&#10;            &lt;/plugin&gt;&#10;&#10;            &lt;!-- Ajout maven-shade pour construire un jar &quot;uber&quot; incluant scala-library et autres dépendances --&gt;&#10;            &lt;plugin&gt;&#10;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#10;                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;&#10;                &lt;version&gt;3.4.1&lt;/version&gt;&#10;                &lt;executions&gt;&#10;                    &lt;execution&gt;&#10;                        &lt;phase&gt;package&lt;/phase&gt;&#10;                        &lt;goals&gt;&#10;                            &lt;goal&gt;shade&lt;/goal&gt;&#10;                        &lt;/goals&gt;&#10;                        &lt;configuration&gt;&#10;                            &lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt;&#10;                            &lt;shadedArtifactAttached&gt;true&lt;/shadedArtifactAttached&gt;&#10;                            &lt;shadedClassifierName&gt;uber&lt;/shadedClassifierName&gt;&#10;                            &lt;filters&gt;&#10;                                &lt;filter&gt;&#10;                                    &lt;artifact&gt;*:*&lt;/artifact&gt;&#10;                                    &lt;excludes&gt;&#10;                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;&#10;                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;&#10;                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;&#10;                                    &lt;/excludes&gt;&#10;                                &lt;/filter&gt;&#10;                            &lt;/filters&gt;&#10;                        &lt;/configuration&gt;&#10;                    &lt;/execution&gt;&#10;                &lt;/executions&gt;&#10;            &lt;/plugin&gt;&#10;&#10;        &lt;/plugins&gt;&#10;    &lt;/build&gt;&#10;&#10;&lt;/project&gt;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/run-spark-etl.ps1">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/run-spark-etl.ps1" />
              <option name="originalContent" value="# Spark ETL Application Runner&#10;# Usage: .\run-spark-etl.ps1&#10;Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;Write-Host &quot;  Spark ETL - Application Execution&quot; -ForegroundColor Cyan&#10;Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;Write-Host &quot;&quot;&#10;# Configure environment variables&#10;$env:HADOOP_HOME = &quot;C:\dev\dev-tools\hadoop-3.3.5&quot;&#10;Write-Host &quot;OK HADOOP_HOME configured: $env:HADOOP_HOME&quot; -ForegroundColor Green&#10;# Check winutils.exe&#10;$winutilsPath = &quot;$env:HADOOP_HOME\bin\winutils.exe&quot;&#10;if (Test-Path $winutilsPath) {&#10;    Write-Host &quot;OK winutils.exe found&quot; -ForegroundColor Green&#10;} else {&#10;    Write-Host &quot;ERROR: winutils.exe not found in $env:HADOOP_HOME\bin&quot; -ForegroundColor Red&#10;    Write-Host &quot;  Download from: https://github.com/cdarlint/winutils&quot; -ForegroundColor Yellow&#10;    exit 1&#10;}&#10;# Check uber JAR&#10;$uberJarPath = &quot;target\scala-spark-etl-1.0-SNAPSHOT-uber.jar&quot;&#10;if (-not (Test-Path $uberJarPath)) {&#10;    Write-Host &quot;ERROR: Uber JAR not found. Compiling...&quot; -ForegroundColor Yellow&#10;    Write-Host &quot;&quot;&#10;    mvn clean package -DskipTests&#10;    if ($LASTEXITCODE -ne 0) {&#10;        Write-Host &quot;ERROR during compilation&quot; -ForegroundColor Red&#10;        exit 1&#10;    }&#10;}&#10;Write-Host &quot;OK Uber JAR found&quot; -ForegroundColor Green&#10;Write-Host &quot;&quot;&#10;# Check input data&#10;$inputPath = &quot;data\input&quot;&#10;if (-not (Test-Path $inputPath)) {&#10;    Write-Host &quot;data/input directory not found. Creating...&quot; -ForegroundColor Yellow&#10;    New-Item -ItemType Directory -Force -Path $inputPath | Out-Null&#10;    # Create sample CSV file&#10;    &quot;id,name,age,city&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8&#10;    &quot;1,John Doe,30,New York&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    &quot;2,Jane Smith,25,Los Angeles&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    &quot;3,Bob Johnson,35,Chicago&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    &quot;4,Alice Williams,28,Houston&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    &quot;5,Charlie Brown,32,Phoenix&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    Write-Host &quot;OK sample.csv created in data/input&quot; -ForegroundColor Green&#10;}&#10;# Count CSV files&#10;$csvFiles = Get-ChildItem -Path $inputPath -Filter &quot;*.csv&quot; -File -ErrorAction SilentlyContinue&#10;Write-Host &quot;OK CSV files found: $($csvFiles.Count)&quot; -ForegroundColor Green&#10;Write-Host &quot;&quot;&#10;# Execute spark-submit&#10;Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;Write-Host &quot;  Launching Spark...&quot; -ForegroundColor Cyan&#10;Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;Write-Host &quot;&quot;&#10;$sparkCommand = &quot;spark-submit --class com.company.etl.spark.Main --master local[*] $uberJarPath&quot;&#10;Write-Host &quot;Command: $sparkCommand&quot; -ForegroundColor Gray&#10;Write-Host &quot;&quot;&#10;Invoke-Expression $sparkCommand&#10;# Check exit code&#10;if ($LASTEXITCODE -eq 0) {&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Green&#10;    Write-Host &quot;  Execution completed successfully!&quot; -ForegroundColor Green&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Green&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Check results in: data\output&quot; -ForegroundColor Cyan&#10;    # Display output files&#10;    if (Test-Path &quot;data\output&quot;) {&#10;        $outputFiles = Get-ChildItem -Path &quot;data\output&quot; -Recurse -File -ErrorAction SilentlyContinue&#10;        if ($outputFiles.Count -gt 0) {&#10;            Write-Host &quot;&quot;&#10;            Write-Host &quot;Generated output files:&quot; -ForegroundColor Cyan&#10;            foreach ($file in $outputFiles) {&#10;                $sizeKB = [math]::Round($file.Length / 1KB, 2)&#10;                $relativePath = $file.FullName.Replace((Get-Location).Path, &quot;.&quot;)&#10;                Write-Host &quot;  - $relativePath ($sizeKB KB)&quot; -ForegroundColor Gray&#10;            }&#10;        }&#10;    }&#10;} else {&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Red&#10;    Write-Host &quot;  Execution error&quot; -ForegroundColor Red&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Red&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Exit code: $LASTEXITCODE&quot; -ForegroundColor Red&#10;    exit $LASTEXITCODE&#10;}&#10;Write-Host &quot;&quot;&#10;" />
              <option name="updatedContent" value="﻿﻿# Spark ETL Application Runner&#10;# Usage: .\run-spark-etl.ps1&#10;Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;Write-Host &quot;  Spark ETL - Application Execution&quot; -ForegroundColor Cyan&#10;Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;Write-Host &quot;&quot;&#10;# Configure environment variables&#10;$env:HADOOP_HOME = &quot;C:\dev\dev-tools\hadoop-3.3.5&quot;&#10;Write-Host &quot;OK HADOOP_HOME configured: $env:HADOOP_HOME&quot; -ForegroundColor Green&#10;# Check winutils.exe&#10;$winutilsPath = &quot;$env:HADOOP_HOME\bin\winutils.exe&quot;&#10;if (Test-Path $winutilsPath) {&#10;    Write-Host &quot;OK winutils.exe found&quot; -ForegroundColor Green&#10;} else {&#10;    Write-Host &quot;ERROR: winutils.exe not found in $env:HADOOP_HOME\bin&quot; -ForegroundColor Red&#10;    Write-Host &quot;  Download from: https://github.com/cdarlint/winutils&quot; -ForegroundColor Yellow&#10;    exit 1&#10;}&#10;# Check uber JAR&#10;$uberJarPath = &quot;target\scala-spark-etl-1.0-SNAPSHOT-uber.jar&quot;&#10;if (-not (Test-Path $uberJarPath)) {&#10;    Write-Host &quot;ERROR: Uber JAR not found. Compiling...&quot; -ForegroundColor Yellow&#10;    Write-Host &quot;&quot;&#10;    mvn clean package -DskipTests&#10;    if ($LASTEXITCODE -ne 0) {&#10;        Write-Host &quot;ERROR during compilation&quot; -ForegroundColor Red&#10;        exit 1&#10;    }&#10;}&#10;Write-Host &quot;OK Uber JAR found&quot; -ForegroundColor Green&#10;Write-Host &quot;&quot;&#10;# Check input data&#10;$inputPath = &quot;data\input&quot;&#10;if (-not (Test-Path $inputPath)) {&#10;    Write-Host &quot;data/input directory not found. Creating...&quot; -ForegroundColor Yellow&#10;    New-Item -ItemType Directory -Force -Path $inputPath | Out-Null&#10;    # Create sample CSV file&#10;    &quot;id,name,age,city&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8&#10;    &quot;1,John Doe,30,New York&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    &quot;2,Jane Smith,25,Los Angeles&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    &quot;3,Bob Johnson,35,Chicago&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    &quot;4,Alice Williams,28,Houston&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    &quot;5,Charlie Brown,32,Phoenix&quot; | Out-File -FilePath &quot;$inputPath\sample.csv&quot; -Encoding utf8 -Append&#10;    Write-Host &quot;OK sample.csv created in data/input&quot; -ForegroundColor Green&#10;}&#10;# Count CSV files&#10;$csvFiles = Get-ChildItem -Path $inputPath -Filter &quot;*.csv&quot; -File -ErrorAction SilentlyContinue&#10;Write-Host &quot;OK CSV files found: $($csvFiles.Count)&quot; -ForegroundColor Green&#10;Write-Host &quot;&quot;&#10;# Add Hadoop bin to PATH for native library access&#10;$env:PATH = &quot;$env:HADOOP_HOME\bin;$env:PATH&quot;&#10;Write-Host &quot;OK PATH updated with Hadoop bin&quot; -ForegroundColor Green&#10;Write-Host &quot;&quot;&#10;&#10;# Setup and clean Spark temp directory&#10;$sparkTempDir = &quot;spark-temp&quot;&#10;Write-Host &quot;Preparing Spark temp directory...&quot; -ForegroundColor Yellow&#10;&#10;# Run cleanup script if it exists&#10;if (Test-Path &quot;.\cleanup-temp.ps1&quot;) {&#10;    # Run cleanup non-interactively (auto-confirm)&#10;    &amp; .\cleanup-temp.ps1 -AutoConfirm -Silent&#10;} else {&#10;    # Fallback: simple cleanup if script not found&#10;    if (Test-Path $sparkTempDir) {&#10;        Remove-Item -Path &quot;$sparkTempDir\*&quot; -Recurse -Force -ErrorAction SilentlyContinue&#10;    }&#10;}&#10;&#10;# Ensure temp directory exists&#10;if (-not (Test-Path $sparkTempDir)) {&#10;    New-Item -ItemType Directory -Force -Path $sparkTempDir | Out-Null&#10;}&#10;Write-Host &quot;OK Spark temp directory ready&quot; -ForegroundColor Green&#10;Write-Host &quot;&quot;&#10;&#10;# Execute spark-submit&#10;Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;Write-Host &quot;  Launching Spark...&quot; -ForegroundColor Cyan&#10;Write-Host &quot;========================================&quot; -ForegroundColor Cyan&#10;Write-Host &quot;&quot;&#10;$sparkCommand = &quot;spark-submit --class com.company.etl.spark.Main --master local[*] --conf `&quot;spark.local.dir=$sparkTempDir`&quot; $uberJarPath&quot;&#10;Write-Host &quot;Command: $sparkCommand&quot; -ForegroundColor Gray&#10;Write-Host &quot;&quot;&#10;Invoke-Expression $sparkCommand&#10;# Check exit code&#10;if ($LASTEXITCODE -eq 0) {&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Green&#10;    Write-Host &quot;  Execution completed successfully!&quot; -ForegroundColor Green&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Green&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Check results in: data\output&quot; -ForegroundColor Cyan&#10;    # Display output files&#10;    if (Test-Path &quot;data\output&quot;) {&#10;        $outputFiles = Get-ChildItem -Path &quot;data\output&quot; -Recurse -File -ErrorAction SilentlyContinue&#10;        if ($outputFiles.Count -gt 0) {&#10;            Write-Host &quot;&quot;&#10;            Write-Host &quot;Generated output files:&quot; -ForegroundColor Cyan&#10;            foreach ($file in $outputFiles) {&#10;                $sizeKB = [math]::Round($file.Length / 1KB, 2)&#10;                $relativePath = $file.FullName.Replace((Get-Location).Path, &quot;.&quot;)&#10;                Write-Host &quot;  - $relativePath ($sizeKB KB)&quot; -ForegroundColor Gray&#10;            }&#10;        }&#10;    }&#10;} else {&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Red&#10;    Write-Host &quot;  Execution error&quot; -ForegroundColor Red&#10;    Write-Host &quot;========================================&quot; -ForegroundColor Red&#10;    Write-Host &quot;&quot;&#10;    Write-Host &quot;Exit code: $LASTEXITCODE&quot; -ForegroundColor Red&#10;    exit $LASTEXITCODE&#10;}&#10;Write-Host &quot;&quot;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>